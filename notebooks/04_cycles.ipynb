{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reusable components for HyperGraph demo notebooks.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "\n",
    "\n",
    "class LLM:\n",
    "    \"\"\"Thin wrapper around OpenAI chat completions.\"\"\"\n",
    "\n",
    "    def __init__(self, model: str = \"gpt-4o-mini\", temperature: float = 0.7):\n",
    "        self.client = AsyncOpenAI()\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "\n",
    "    async def generate(self, messages: list[dict]) -> str:\n",
    "        resp = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        return resp.choices[0].message.content or \"\"\n",
    "\n",
    "    async def answer_with_context(self, query: str, documents: list[str]) -> str:\n",
    "        context = \"\\n\".join(f\"- {d}\" for d in documents)\n",
    "        return await self.generate(\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Answer based ONLY on the provided context. Be concise.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\",\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    async def judge(self, query: str, expected: str, actual: str) -> str:\n",
    "        return await self.generate(\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a judge. Given a question, expected answer, and actual answer, \"\n",
    "                        \"rate the actual answer from 1-5 and explain why. \"\n",
    "                        \"Respond in the format: SCORE: X\\nREASON: ...\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Question: {query}\\nExpected: {expected}\\nActual: {actual}\",\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "class Embedder:\n",
    "    \"\"\"Wraps OpenAI embeddings API.\"\"\"\n",
    "\n",
    "    def __init__(self, model: str = \"text-embedding-3-small\"):\n",
    "        self.client = OpenAI()\n",
    "        self.model = model\n",
    "\n",
    "    def embed(self, text: str) -> list[float]:\n",
    "        return self.embed_batch([text])[0]\n",
    "\n",
    "    def embed_batch(self, texts: list[str]) -> list[list[float]]:\n",
    "        resp = self.client.embeddings.create(input=texts, model=self.model)\n",
    "        return [d.embedding for d in resp.data]\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"In-memory vector store using cosine similarity.\"\"\"\n",
    "\n",
    "    def __init__(self, documents: list[str], embedder: Embedder):\n",
    "        self.documents = documents\n",
    "        self.embeddings = embedder.embed_batch(documents)\n",
    "\n",
    "    def search(self, query_embedding: list[float], top_k: int = 5) -> list[dict]:\n",
    "        scores = [_cosine_similarity(query_embedding, de) for de in self.embeddings]\n",
    "        ranked = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "        return [{\"text\": self.documents[i], \"score\": scores[i]} for i in ranked[:top_k]]\n",
    "\n",
    "\n",
    "def _cosine_similarity(a: list[float], b: list[float]) -> float:\n",
    "    va, vb = np.array(a), np.array(b)\n",
    "    return float(np.dot(va, vb) / (np.linalg.norm(va) * np.linalg.norm(vb)))\n",
    "\n",
    "\n",
    "DOCUMENTS = [\n",
    "    \"HyperGraph is a Python framework for building AI/ML workflows using explicit graphs.\",\n",
    "    \"HyperGraph supports hierarchy — graphs that contain graphs — for managing complexity.\",\n",
    "    \"The .map() feature lets you write code for a single item and scale to many automatically.\",\n",
    "    \"HyperGraph unifies DAGs and cycles in one framework, enabling both pipelines and agents.\",\n",
    "    \"Nodes in HyperGraph are pure functions. Edges are inferred from matching output→input names.\",\n",
    "    \"Python was created by Guido van Rossum and first released in 1991.\",\n",
    "    \"The capital of France is Paris.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypergraph import END, AsyncRunner, Graph, interrupt, node\n",
    "\n",
    "\n",
    "@node(output_name=\"query_embedding\")\n",
    "def embed_query(query: str, embedder) -> list[float]:\n",
    "    return embedder.embed(query)\n",
    "\n",
    "\n",
    "@node(output_name=\"documents\")\n",
    "def retrieve(query_embedding: list[float], vector_store) -> list[str]:\n",
    "    return [d[\"text\"] for d in vector_store.search(query_embedding, top_k=3)]\n",
    "\n",
    "\n",
    "@node(output_name=\"response\")\n",
    "async def generate(query: str, documents: list[str], messages: list[dict], llm) -> str:\n",
    "    context = \"\\n\".join(f\"- {d}\" for d in documents)\n",
    "    chat_messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Answer based on the provided context. Be concise.\",\n",
    "        },\n",
    "        *messages,\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"},\n",
    "    ]\n",
    "    return await llm.generate(chat_messages)\n",
    "\n",
    "\n",
    "rag_graph = Graph([embed_query, retrieve, generate], name=\"rag\")\n",
    "embedder = Embedder()\n",
    "llm = LLM()\n",
    "vector_store = VectorStore(DOCUMENTS, embedder)\n",
    "rag_graph = rag_graph.bind(embedder=embedder, llm=llm, vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@node(output_name=\"messages\", hide=True)\n",
    "def add_user_message(messages: list[dict], query: str) -> list[dict]:\n",
    "    return [\n",
    "        *messages,\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "\n",
    "\n",
    "@node(output_name=\"messages\", hide=True)\n",
    "def add_assistant_message(messages: list[dict], response: str) -> list[dict]:\n",
    "    return [\n",
    "        *messages,\n",
    "        {\"role\": \"assistant\", \"content\": response},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interrupt(output_name=\"query\")\n",
    "def ask_user(response: str, messages: list[dict]) -> str | None:\n",
    "    return None  # always pause for human input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypergraph import ifelse\n",
    "\n",
    "\n",
    "@ifelse(when_true=\"rag\", when_false=END)\n",
    "def should_continue(query: str) -> bool:\n",
    "    return query.strip().lower() == \"exit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_graph = Graph(\n",
    "    [ask_user, add_user_message, rag_graph.as_node(name=\"rag\"), add_assistant_message],\n",
    "    edges=[\n",
    "        (ask_user, add_user_message),       # query\n",
    "        (ask_user, \"rag\"),                   # query\n",
    "        (add_user_message, \"rag\"),           # messages\n",
    "        (\"rag\", add_assistant_message),      # response\n",
    "        (add_assistant_message, ask_user),   # messages\n",
    "        (\"rag\", ask_user),                   # response\n",
    "    ],\n",
    "    name=\"rag_chat\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_graph.visualize(depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive chat loop\n",
    "\n",
    "The runner pauses at `ask_user`, we collect input, then resume.\n",
    "Type `exit` to end the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = AsyncRunner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run — provide initial query and empty history\n",
    "result = await runner.run(\n",
    "    chat_graph,\n",
    "    {\"query\": \"What is HyperGraph?\", \"messages\": []},\n",
    "    max_iterations=50,\n",
    "    select=[\"response\"],\n",
    ")\n",
    "print(f\"A: {result['response']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation interactively\n",
    "while result.paused:\n",
    "    user_input = input(\"You: \")\n",
    "    result = await runner.run(\n",
    "        chat_graph,\n",
    "        {result.pause.response_key: user_input, \"messages\": result.pause.value},\n",
    "        max_iterations=50,\n",
    "    )\n",
    "    if not result.paused:\n",
    "        break\n",
    "    print(f\"A: {result['response']}\\n\")\n",
    "\n",
    "print(\"\\nGoodbye!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
