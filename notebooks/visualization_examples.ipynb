{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypergraph Visualization Examples\n",
    "\n",
    "This notebook demonstrates the `graph.visualize()` method across different graph patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypergraph import END, Graph, ifelse, node, route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Single Node\n",
    "\n",
    "The simplest case - one function node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@node(output_name=\"doubled\")\n",
    "def double(x: int) -> int:\n",
    "    \"\"\"Double a number.\"\"\"\n",
    "    return x * 2\n",
    "\n",
    "\n",
    "graph = Graph(nodes=[double])\n",
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Pipeline\n",
    "\n",
    "A simple DAG where data flows through multiple nodes in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@node(output_name=\"cleaned\")\n",
    "def clean(raw_text: str) -> str:\n",
    "    \"\"\"Clean raw text.\"\"\"\n",
    "    return raw_text.strip().lower()\n",
    "\n",
    "\n",
    "@node(output_name=\"tokens\")\n",
    "def tokenize(cleaned: str) -> list[str]:\n",
    "    \"\"\"Tokenize cleaned text.\"\"\"\n",
    "    return cleaned.split()\n",
    "\n",
    "\n",
    "@node(output_name=\"embedding\")\n",
    "def embed(tokens: list[str]) -> list[float]:\n",
    "    \"\"\"Create embedding from tokens.\"\"\"\n",
    "    return [0.1] * len(tokens)\n",
    "\n",
    "\n",
    "pipeline = Graph(nodes=[clean, tokenize, embed])\n",
    "pipeline.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multiple Inputs\n",
    "\n",
    "A node that takes multiple inputs from different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@node(output_name=\"query_vec\")\n",
    "def embed_query(query: str) -> list[float]:\n",
    "    \"\"\"Embed the query.\"\"\"\n",
    "    return [0.1] * 10\n",
    "\n",
    "\n",
    "@node(output_name=\"docs\")\n",
    "def retrieve(query_vec: list[float], top_k: int) -> list[str]:\n",
    "    \"\"\"Retrieve documents.\"\"\"\n",
    "    return [\"doc1\", \"doc2\"]\n",
    "\n",
    "\n",
    "@node(output_name=\"answer\")\n",
    "def generate(docs: list[str], query: str) -> str:\n",
    "    \"\"\"Generate answer from docs and query.\"\"\"\n",
    "    return f\"Answer based on {len(docs)} docs\"\n",
    "\n",
    "\n",
    "rag = Graph(nodes=[embed_query, retrieve, generate])\n",
    "rag.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. With Type Annotations\n",
    "\n",
    "Use `show_types=True` to display input/output types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.visualize(show_types=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bound Inputs\n",
    "\n",
    "Inputs that have been bound to specific values appear differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind top_k to 5\n",
    "rag_bound = rag.bind(top_k=5)\n",
    "rag_bound.visualize(show_types=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Binary Branching with @ifelse\n",
    "\n",
    "Conditional routing based on a boolean condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@node(output_name=\"cached_result\")\n",
    "def use_cache(query: str) -> str:\n",
    "    \"\"\"Return cached result.\"\"\"\n",
    "    return \"cached answer\"\n",
    "\n",
    "\n",
    "@node(output_name=\"fresh_result\")\n",
    "def compute_fresh(query: str) -> str:\n",
    "    \"\"\"Compute fresh result.\"\"\"\n",
    "    return \"fresh answer\"\n",
    "\n",
    "\n",
    "@ifelse(when_true=\"use_cache\", when_false=\"compute_fresh\")\n",
    "def check_cache(query: str) -> bool:\n",
    "    \"\"\"Check if query is in cache.\"\"\"\n",
    "    return query in [\"hello\", \"world\"]\n",
    "\n",
    "\n",
    "branching = Graph(nodes=[check_cache, use_cache, compute_fresh])\n",
    "branching.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-way Routing with @route\n",
    "\n",
    "Route to different nodes based on string return value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@node(output_name=\"small_result\")\n",
    "def process_small(data: str) -> str:\n",
    "    return \"processed small\"\n",
    "\n",
    "\n",
    "@node(output_name=\"medium_result\")\n",
    "def process_medium(data: str) -> str:\n",
    "    return \"processed medium\"\n",
    "\n",
    "\n",
    "@node(output_name=\"large_result\")\n",
    "def process_large(data: str) -> str:\n",
    "    return \"processed large\"\n",
    "\n",
    "\n",
    "@route(targets=[\"process_small\", \"process_medium\", \"process_large\"])\n",
    "def classify_size(data: str) -> str:\n",
    "    \"\"\"Classify data by size.\"\"\"\n",
    "    if len(data) < 10:\n",
    "        return \"process_small\"\n",
    "    elif len(data) < 100:\n",
    "        return \"process_medium\"\n",
    "    return \"process_large\"\n",
    "\n",
    "\n",
    "routing = Graph(nodes=[classify_size, process_small, process_medium, process_large])\n",
    "routing.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cyclic Graph (Agentic Loop)\n",
    "\n",
    "A graph with cycles for iterative processing. Uses `END` sentinel to break out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@node(output_name=\"response\")\n",
    "def generate_response(messages: list[dict]) -> str:\n",
    "    \"\"\"Generate LLM response.\"\"\"\n",
    "    return \"I can help with that!\"\n",
    "\n",
    "\n",
    "@node(output_name=\"messages\")\n",
    "def accumulate(messages: list[dict], response: str) -> list[dict]:\n",
    "    \"\"\"Add response to message history.\"\"\"\n",
    "    return messages + [{\"role\": \"assistant\", \"content\": response}]\n",
    "\n",
    "\n",
    "@route(targets=[\"generate_response\", END])\n",
    "def should_continue(messages: list[dict]) -> str:\n",
    "    \"\"\"Decide whether to continue the conversation.\"\"\"\n",
    "    if len(messages) > 5:\n",
    "        return END\n",
    "    return \"generate_response\"\n",
    "\n",
    "\n",
    "agent_loop = Graph(nodes=[generate_response, accumulate, should_continue])\n",
    "agent_loop.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Nested Graph (Hierarchical Composition)\n",
    "\n",
    "Graphs can contain other graphs as nodes. Use `depth` to control expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner graph: text processing pipeline\n",
    "@node(output_name=\"cleaned\")\n",
    "def clean_text(text: str) -> str:\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "@node(output_name=\"normalized\")\n",
    "def normalize(cleaned: str) -> str:\n",
    "    return cleaned.lower()\n",
    "\n",
    "\n",
    "preprocess = Graph(nodes=[clean_text, normalize], name=\"preprocess\")\n",
    "\n",
    "\n",
    "# Outer graph using the inner graph\n",
    "@node(output_name=\"result\")\n",
    "def analyze(normalized: str) -> dict:\n",
    "    return {\"length\": len(normalized)}\n",
    "\n",
    "\n",
    "workflow = Graph(nodes=[preprocess.as_node(), analyze])\n",
    "print(\"depth=0 (collapsed):\")\n",
    "workflow.visualize(depth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"depth=1 (expanded):\")\n",
    "workflow.visualize(depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Multi-level Nesting\n",
    "\n",
    "Graphs can be nested multiple levels deep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 1: Simple transform\n",
    "@node(output_name=\"step1_out\")\n",
    "def step1(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "@node(output_name=\"step2_out\")\n",
    "def step2(step1_out: int) -> int:\n",
    "    return step1_out * 2\n",
    "\n",
    "\n",
    "inner = Graph(nodes=[step1, step2], name=\"inner\")\n",
    "\n",
    "\n",
    "# Level 2: Wrap inner + add validation\n",
    "@node(output_name=\"validated\")\n",
    "def validate(step2_out: int) -> int:\n",
    "    return step2_out\n",
    "\n",
    "\n",
    "middle = Graph(nodes=[inner.as_node(), validate], name=\"middle\")\n",
    "\n",
    "\n",
    "# Level 3: Wrap middle + add logging\n",
    "@node(output_name=\"logged\")\n",
    "def log_result(validated: int) -> int:\n",
    "    print(f\"Result: {validated}\")\n",
    "    return validated\n",
    "\n",
    "\n",
    "outer = Graph(nodes=[middle.as_node(), log_result])\n",
    "\n",
    "print(\"depth=0:\")\n",
    "outer.visualize(depth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"depth=1:\")\n",
    "outer.visualize(depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"depth=2 (fully expanded):\")\n",
    "outer.visualize(depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Theme Options\n",
    "\n",
    "Force light or dark theme regardless of environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Light theme:\")\n",
    "pipeline.visualize(theme=\"light\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dark theme:\")\n",
    "pipeline.visualize(theme=\"dark\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
