I thought that we already implemented something that allows to have multiple similar output names correctly, but i recently saw that it only takes the first source or target from it. can you look at the example with ask user -> query -> add query to messages -> generate -> add response to messages etc... - find where this example is defined and show it to me and let's see if it handles messages correctly?

---

can you show me where it's written? this example

---

BTW - why do we have a purple dot at the top right of retrieve and generate?

---

---------------------------------------------------------------------------
GraphConfigError                          Traceback (most recent call last)
Cell In[8], line 1
----> 1 chat_graph = Graph(
      2     [ask_user, rag_graph.as_node(name="rag"), add_user_message, add_assistant_message],
      3     name="rag_chat",
      4 )

File ~/python_workspace/hypergraph/src/hypergraph/graph/core.py:95, in Graph.__init__(self, nodes, name, strict_types)
     93 self._selected: tuple[str, ...] | None = None
     94 self._nodes = self._build_nodes_dict(nodes)
---> 95 self._nx_graph = self._build_graph(nodes)
     96 self._cached_hash: str | None = None
     97 self._controlled_by: dict[str, list[str]] | None = None

File ~/python_workspace/hypergraph/src/hypergraph/graph/core.py:259, in Graph._build_graph(self, nodes)
    255 self._add_ordering_edges(G, nodes, output_to_source)
    257 # Validate output conflicts with full graph structure
    258 # This allows mutex branch detection using reachability analysis
--> 259 validate_output_conflicts(G, nodes, output_to_sources)
    261 return G

File ~/python_workspace/hypergraph/src/hypergraph/graph/_conflict.py:66, in validate_output_conflicts(G, nodes, output_to_sources)
     63 if _is_pair_ordered(a, b, all_contested, node_names, edge_map):
     64     continue
---> 66 raise GraphConfigError(
     67     f"Multiple nodes produce '{output}'\n\n"
     68     f"  -> {a} creates '{output}'\n"
     69     f"  -> {b} creates '{output}'\n\n"
     70     f"How to fix:\n"
     71     f"  - Add ordering with emit/wait_for between the producers\n"
     72     f"  - Or place them in exclusive gate branches"
     73 )

GraphConfigError: Multiple nodes produce 'messages'

  -> add_user_message creates 'messages'
  -> add_assistant_message creates 'messages'

How to fix:
  - Add ordering with emit/wait_for between the producers
  - Or place them in exclusive gate branches

---

I think it gets too messy. Let's fix a couple of things together in this session. please open a new branch and worktree for explicit-edges

---

can you switch to there?

---

here's my raw conversation with claude while driving, can you suggest what we should focus on based on that? Hey Gilad, how's it going? What's on your mind today?

I want to think together about something in my hypergraph project.

Feb 17
Absolutely, let's dig into it. What's on your mind with HyperGraph?

So I have a new feature that enables running a DAG from anywhere in the graph depending on the inputs you provide to the graph.

Feb 17
Right, that's a neat capability—sounds like it gives you flexibility to trigger subgraphs dynamically. What's the conceptual challenge you're wrestling with, or are you thinking through the implementation?

So the point is that if, let's say, there are, uh, three nodes, one after the other, a, b, and c, and, um, I provide the input to node c, it should run node c. And that is supposed to work today. But what if the user provides inputs to node a and node c? What should happen there? How should I handle this case?

Feb 17
Right, that's a really interesting design problem. So you've got partial inputs scattered across the graph, and you need to decide what actually runs.

d say the key question is: what's the user's intent when they provide inputs to both a and c? Are they saying "run both of these independently," or are they expecting some kind of unified execution? Because those lead to different solutions.

One approach is to treat it as "run all nodes that have their inputs satisfied" — so a runs because it has inputs, then c runs because it has inputs. But that breaks the DAG dependency chain, yeah? Node c might depend on outputs from b, which depends on a, so you'd need a's output to properly run c.

The other angle is to ask: should providing inputs to non-consecutive nodes be allowed at all? Maybe the constraint is "you can only provide inputs to a contiguous slice of the graph," which keeps things predictable.

What's your intuition here — are users likely to do this intentionally, or is it more of an edge case you're trying to handle defensively?

Yeah. No. That helped, actually. It's helped. Uh, it's it's an error. It... I should error. There isn't any dilemmae. Uh, another another point I want to talk about is entry points. If I have a graph that is cyclic and, um, if I allow... if I run the graph based on the inputs, like, whatever nodes are satisfied, essentially, I give the user the freedom to choose where to start the execution of the cyclic graph. And do you understand that?

Feb 17
Yeah, I follow. So with a cyclic graph, you're saying the user could provide inputs to different nodes, and whichever nodes have their dependencies satisfied can start executing right away, rather than being locked into a single entry point. That gives them flexibility in where the computation actually kicks off depending on what data they've got available. Is that right?

Yes. Exactly. What I'm afraid is that, like, for some... it it might be a little bit less intuitive for some people about, like... they want to start with an ask user node, and so they need to provide the message history and a response perhaps. Or, like, in in... let's say I have a graph that has an ask ask user, uh, node, and then it goes into add, uh, query to messages, and then it sends messages into generate. And then generate goes into add response to messages, and then this goes back into the ask user as kind of like a, um, like a a signal to say, okay. Now you need to ask the user again.

Feb 17
Ah, I see the challenge here. So you've got this cyclical agentic loop, and if someone wants to jump in at the ask user node, they'd need to provide all the upstream state—the message history, the response from the previous generation—just to satisfy that node's inputs. That's the friction point, yeah?

So the question is really about discoverability and usability. Users might not immediately realize what inputs they actually need to provide to kick off execution at their chosen entry point. Is that the core concern, or are you also thinking about whether it's even the right mental model to let them choose arbitrary entry points in a cyclic graph?

Let's continue with this example. Let's say, the ask user, umesn't require any... actually, it doesn't really require any inputs. Right? Uh, but it does require a signal that's, like, within a graph, within a cycle. Uh, it it should start after, um, the the response has been received or after the respond has been added to the messages. So, actually, hypergraph supports this. Um, so, essentially, if the the user wants to start with ask user, they don't need to provide any input to the graph. But then after ask user, it goes into add query to messages, and this requires a messages list. So the user needs to know what what inputs are needed in order to continue or to start the graph from there even if the starting node doesn't have this requirement.

Feb 17
Ah, I see the complexity now. So the intuition problem is: the user says "I want to start at ask user," but ask user itself doesn't need inputs—it just needs a signal from the cycle. But then the next node in the chain needs the messages list, which the user has to know about and provide, even though ask user neversked for it.

So there's a mismatch between where the user thinks they're starting and what inputs they actually need to provide to keep the graph flowing. They're not directly feeding ask user—they're feeding the downstream nodes that depend on its output.

Is the problem that this is unintuitive because the user has to reason backwards from ask user through the whole execution chain to figure out what initial inputs are actually required? Like, they'd need to understand the full graph topology to know "okay, if I want to start here, I actually need to bootstrap these inputs downstream."

Exactly.

Feb 17
Yeah, so that's a real UX problem. Users would need to mentally trace through the entire graph topology just to figure out what they need to provide upfront, which defeats the purpose of having flexible entry points.

I think the answer might be to make this explicit in the API. When a user specifies their entry point, you could automatically compute and surface the required inputs—like, "you want to t at ask user, so you'll need to provide messages list, and here's why." That way the user doesn't have to figure it out themselves; the framework tells them what's actually needed given their chosen entry point. Does that feel like it'd solve the intuitiveness problem?

Yes. But it... it's kind of, like, um, intermingled with another, uh, topic I'm, uh, I'm struggling with, uh, with regards to Hypergraph, which is that today, the default is that, uh, edges are inferred automatically by the names of the outputs and the inputs into the sick... like, the signature of the functions and the output name. But in cyclic graphs, in in example... in in our example, we have the messages list, which updates, uh, and goes into certain, uh, nodes and and, like, we have add query to messages and add response to messages and all these things. And in order for this to work, I need to create, um, uh, like, a... instead of messages, I need to create a messages with query, uh, and then another one that's messages with response in order to, um, define the the right ordering of of the graph. And this is... first of all, I don't really like it so much, and it makes things less intuitive. And I'm thinking, uh, to, uh, enable, uh, where it's possible, like, a... an explicit edges graph. Like, you can switch modes, the default in tags or whatever is, uh, automatic in, uh, inference of edges, but you can switch this thing off and, um, either override or manually define all the the edges. And in in this way, you you go into a different mode of operation. What do you think about that?

Show more
Feb 17
That's a smart move, honestly. The auto-wiring is great for simple DAGs where the naming conventions naturally align, but cyclic graphs with the same data type flowing through multiple transformation steps—yeah, that breaks down quickly. You end up fighting the inference instead of using it.

I like the idea of an explicit edges mode. It gives you clarity without forcing everyone into manual wiring from day one. The question in my head : how does the mode switching feel from a user perspective? Are they defining the whole graph differently, or is it more like "I'm going to manually override these three edges while the rest auto-wire"? Because if it's all-or-nothing, that might still feel clunky for mixed scenarios.

What's your instinct—full explicit mode, or selective overrides on top of auto-wiring?

Let's take the example I gave before, and I want to add that I currently support something called emit and wait for. And, uh, within the decorator of the function, you put at sign and then node, and then you write output name equals blah blah blah, comma, and then you can say emit or... and or wait for. So for example, uh, when we have the, uh, generate function, um, we can say emit, uh, turn is over, and the ask user query can wait for churn is over. And this is how you don't need the function ask user to have any inputs because it doesn't need any inputs. It just needs to wait for this signal. So that's, like, a a way for automatic edgedge inference to handle cycles in this way. So let's take the auto inference in this graph and and try to write it or to to think how it would look like with explicit edges, and then tell me what you think about the two options, mixing them, etcetera.

Feb 17
Right, so let me think through this. With auto-inference and emit/wait for, you're handling the cycle by saying "generate emits turn_is_over, and ask_user waits for turn_is_over"—that solves the naming collision problem where messages gets updated multiple times.

But with explicit edges, you'd wire it manually: ask_user connects to add_query_to_messages, add_query_to_messages connects to generate, generate connects to add_response_to_messages, add_response_to_messages loops back to ask_user. No naming gymnastics needed, the graph topology is crystal clear.

The trade-off I'm seeing is: auto-inference with emit/wait is more compact and lets you keep function signatures clean—you don't need dummy parameters. But it requires understanding this signalpattern, and there's implicit logic in the decorators. Explicit edges are more verbose but the graph structure is right there in front of you, no hidden signals to reason about.

Honestly? I'd lean towards supporting both, but making explicit edges the recommendation for cyclic graphs. Auto-inference is great for simple DAGs where you can just name things well. But once you've got cycles, the explicit edges mode cuts through the intuition problem entirely—users see exactly what's connected to what, and there's no mystery about entry points or downstream dependencies.

Does that feel right?

First of all, in order to actually clarify the issue with auto inference, in order for this graph to work well, I would need... I can't use the the the output name messages because if so, it will create a lot of ambiguity and, like, self loops and things like that. In order to handle that, I need to create messages with query and messages with response, and this is how I solve this, like, these weird cases that happen  I only use messages. So that's the issue, uh, with, um, one of the issues with cycles and auto inference. Now back to the explicit edges. Let's say we do use messages, and we have explicit edges. How do we... how does the, like, entry point thing start here? How how do you imagine a user starting from the entry point, ask user where we have explicit edges?

Feb 17
Okay, so with explicit edges and the same output name messages everywhere, the graph structure is unambiguous—you've explicitly defined which messages output from add query goes where, which messages output from add response goes where, all of it.

So from a user perspective, if they want to start at ask user with explicit edges, they'd... well, they don't need to provide any inputs to ask user itself because it waits for the turn is over signal. But here's the thing: ask user outputs a message, and that feeds into add query to messages. So add query to messages needs that message as input, right? But it also needs the existing messages list toppend to.

So I think the entry point question becomes clearer with explicit edges: you ask the user "what inputs do you need to provide to bootstrap the graph at your chosen entry point?" And the answer isn't "figure out the whole chain backwards"—it's "what are all the nodes that have no incoming edges, or whose incoming edges you're not providing data for?" Those are your actual required inputs.

Does that feel right? Like, explicit edges make it clearer what's actually a gap in the data flow versus what's satisfied by the cycle or by previous nodes in the execution path?

So I think you got it a bit wrong. Uh, I think now that you're talking about this, um, I think that it doesn't matter if we use explicit edges or automatically inferred ones. The only difference is that it allows for, uh, um, using the same output name without worrying about this. Uh, I think that's the only benefit, if I'm not mistaken. What do you think about this?

Feb 17
Yeah, you're right. The explicit edges don't actually solvehe entry point problem—they just let you use cleaner names like messages instead of messages_with_query and messages_with_response. The fundamental issue of the user needing to know what inputs to provide downstream still exists either way. So explicit edges is really about code cleanliness and avoiding naming gymnastics in cyclic graphs, not about solving the intuition problem around entry points. Is that fair?

Yes. And what do you think about mixing auto inference and explicit edges? In in our example, can we, uh, can we make it work with messages somehow?

Feb 17
Hmm, yeah actually—so if you allow mixing modes, you could do something like: keep the auto inference for the straightforward parts of the graph, but use explicit edges just for the cyclic bits where ambiguity creeps in. So in your example, you'd let add query to messages and add response to messages auto-infer their inputs from the messages parameter, but then you'd explicitly define the edges that route the messages output back into ask uor control the cycle ordering. That way you get the naming clarity of using just "messages" without the naming conflicts, because the explicit edges disambiguate which messages goes where in the cycle.

Does that feel like it could work?

I think it could create a lot of edge cases, and I think now a user can either choose this or that. I think that would be easier.

Feb 17
Right, so you're saying keep them separate as two distinct modes rather than trying to blend them. That makes sense—mixing them could get messy really quickly with conflicting rules about which edges take precedence, when naming matters, when it doesn't, all that.

So the user chooses upfront: either I'm using auto-inference with my naming conventions and emit/wait-for signals, or I'm going explicit edges where I wire everything manually and naming doesn't constrain me. Cleaner mental model, fewer edge cases to debug.

For your cyclic messages example, explicit edges would let you just use "messages" throughout without the query/respon suffix workaround. Do you think most users would naturally gravitate toward one mode, or do you imagine projects needing both?

I don't know, but this this really helps. Um, Yeah. Thank you.

Feb 17
Happy to think through

So here's another question. While we're on the topic, um, let's say we have a cyclic graph and no entry point is defined, and then the user, uh, inputs some inputs. Um, but now let's say, like... so I'm just trying to understand, like we said earlier, like, uh, in order to start with ask query, it might... it will fail if messages or messages with response or something like that is not initialized. Um, but that's, like, downstream of of ask user. So first of all, I'm just wondering if we should automatically, like, work warn before running, or we should let it fail because, actually, like, I I think I think we we should, uh, fail fast instead of letting, um, the graph reach that point and then fail because we know it won't work.

Feb 17
Hmm, yeah. So you're thinking: instead of just erroring, proactively show the user all the viable entry points and what inputs each one would require? That's actually really helpful from a UX perspective—it turns a cryptic error into actionable guidance.

But I'd push back slightly: that could get overwhelming if the graph is complex with lots of possible entry points. Maybe the better move is to fail fast with a clear error message that says something like "No entry point specified. Based on your inputs, you could start from nodes X, Y, or Z. Pick one and we'll tell you what else you need to provide." Then they choose, and then you validate the full bootstrap inputs for that path?

That way you're not drowning them in information upfront, but you're guiding them toward a solution rather than just saying "nope, doesn't work."

What do you reckon?

Yeah. I like that. So

Feb 17
Right, so you're asking: given a user chooses "ask user" as their entry point, how do we figure out what inputs they actually need to provide?

I'd say you need to do a reachabilitynalysis from that entry point. You trace forward through the graph from ask user, and you collect all the inputs that those downstream nodes need that aren't satisfied by outputs from nodes you'll execute along the way. So you're essentially asking: "If I start here and let execution flow naturally, what gaps are there?"

In your example, ask user has no inputs, but add query to messages needs messages. Messages isn't produced by ask user, so it becomes a required input. Then add query produces messages with query, which feeds into generate, and so on. You trace through until the cycle closes back, and whatever inputs are still unsatisfied at the end—those are what the user needs to provide upfront.

Does that match what you're thinking, or am I missing a nuance?

No. This is great. Um, now I want to add another complication, which is very interesting. Um, we do have in hypergraph conditional notes. So if a response contains something, then we go here. If not, we go there. And when we have that... so Whatf, like, one of those, uh, routes has a missing input? Should we collect all the required inputs to all, uh, possible, uh, paths? Does that make sense?

Feb 17
Yeah, that's thorny. So you've got branching logic, and one branch might need inputs that the other doesn't. Do you collect inputs for all possible paths upfront, or do you only ask for what's needed for the path that actually executes?

I'd lean toward: only ask for inputs that the executed path needs. Because asking for inputs for paths that won't run feels wasteful and confusing for the user. But the tricky bit is you don't know which path will execute until runtime, right? So you'd have to execute conditionally and then backtrack if you hit a missing input partway through.

Or... do you think there's value in the user explicitly saying "I want to go down this branch," so you know which path they're taking and can validate just that path's inputs upfront?

Hello. Hello. Hello. Hello. Hello.

Feb 17
Sorry, I'm here! What were you saying?

So, um, yeah, the point is that this conditional, uh, node is dependent on runtime execution... or runtime, uh, values. So I can't know this in advance. So as I was asking you, I realized, yes, we do need to collect all the inputs from all the possible branches because, uh, if the user doesn't know what's going to be upfront, then we need to make sure that it doesn't fail, um, if the... if one of the routes is being selected So we have to collect all of them.

Feb 17
Exactly right. You can't know at graph definition time which branch executes, so you have to be pessimistic and say "give me everything that could be needed," even if only one path actually runs. Otherwise you'd hit runtime failures partway through execution when you discover a missing input on the chosen branch.

Can you summarize all the things we talked about so far? Like, the dilemma. Like, you can... can you make a list of, like, dilemma and then decision that I shared with you so far?

Feb 17
Sure, let me walk through what we've covered. First, the partial inputs issue: you realized that if a user provides inputs to non-consecutive nodes in a DAG, that's just an error—no dilemma there, you should error out.

Then we got into entry points in cyclic graphs. The core tension is that users think intuitively about starting from a node like ask user, but they don't realize they need to provide inputs for downstream nodes that ask user feeds into. So you decided to help users by showing them all possible entry points and what inputs each one requires.

Next was the auto-inference versus explicit edges question. You realized that the main benefit of explicit edges is just that it lets you reuse output names like messages without creating ambiguity—whereas auto-inference forces you to create variants like messages with query and messages with response. You decided it's cleaner to let users pick one mode or the other rather than mixing them.

Then came the fail-fast principle: if a user picks an entry point but hasn't provided all necessary downstream inp you should warn them upfront instead of letting the graph run and fail partway through.

And finally, the conditional nodes complication: since you can't know at setup time which branches will execute, you need to collect required inputs from all possible paths so nothing fails at runtime.

Does that capture it?

;;;; I want to tackle everything that we've talked about together

---

yes, great. can you work with /codex-review ?

---

Base directory for this skill: /Users/giladrubin/.claude/skills/codex-review


---
name: codex-review
description: Send the current plan to OpenAI Codex CLI for iterative review. Claude and Codex go back-and-forth until Codex approves the plan.
user_invocable: true
---

# Codex Plan Review (Iterative)

Send the current implementation plan to OpenAI Codex for review. Claude revises the plan based on Codex's feedback and re-submits until Codex approves. Max 5 rounds.

---

## When to Invoke

- When the user runs `/codex-review` during or after plan mode
- When the user wants a second opinion on a plan from a different model

## Agent Instructions

When invoked, perform the following iterative review loop:

### Step 1: Generate Session ID

Generate a unique ID to avoid conflicts with other concurrent Claude Code sessions:

```bash
REVIEW_ID=$(uuidgen | tr '[:upper:]' '[:lower:]' | head -c 8)
```

Use this for all temp file paths: `/tmp/claude-plan-${REVIEW_ID}.md` and `/tmp/codex-review-${REVIEW_ID}.md`.

### Step 2: Capture the Plan

Write the current plan to the session-scoped temporary file. The plan is whatever implementation plan exists in the current conversation context (from plan mode, or a plan discussed in chat).

1. Write the full plan content to `/tmp/claude-plan-${REVIEW_ID}.md`
2. If there is no plan in the current context, ask the user what they want reviewed

### Step 3: Initial Review (Round 1)

Run Codex CLI in non-interactive mode to review the plan:

```bash
codex exec \
  -m gpt-5.3-codex \
  -s read-only \
  -o /tmp/codex-review-${REVIEW_ID}.md \
  "Review the implementation plan in /tmp/claude-plan-${REVIEW_ID}.md. Focus on:
1. Correctness - Will this plan achieve the stated goals?
2. Risks - What could go wrong? Edge cases? Data loss?
3. Missing steps - Is anything forgotten?
4. Alternatives - Is there a simpler or better approach?
5. Security - Any security concerns?

Be specific and actionable. If the plan is solid and ready to implement, end your review with exactly: VERDICT: APPROVED

If changes are needed, end with exactly: VERDICT: REVISE"
```

**Capture the Codex session ID** from the output line that says `session id: <uuid>`. Store this as `CODEX_SESSION_ID`. You MUST use this exact ID to resume in subsequent rounds (do NOT use `--last`, which would grab the wrong session if multiple reviews are running concurrently).

**Notes:**
- Use `-m gpt-5.3-codex` as the default model (configured in `~/.codex/config.toml`). If the user specifies a different model (e.g., `/codex-review o4-mini`), use that instead.
- Use `-s read-only` so Codex can read the codebase for context but cannot modify anything.
- Use `-o` to capture the output to a file for reliable reading.

### Step 4: Read Review & Check Verdict

1. Read `/tmp/codex-review-${REVIEW_ID}.md`
2. Present Codex's review to the user:

```
## Codex Review — Round N (model: gpt-5.3-codex)

[Codex's feedback here]
```

3. Check the verdict:
   - If **VERDICT: APPROVED** → go to Step 7 (Done)
   - If **VERDICT: REVISE** → go to Step 5 (Revise & Re-submit)
   - If no clear verdict but feedback is all positive / no actionable items → treat as approved
   - If max rounds (5) reached → go to Step 7 with a note that max rounds hit

### Step 5: Revise the Plan

Based on Codex's feedback:

1. **Revise the plan** — address each issue Codex raised. Update the plan content in the conversation context and rewrite `/tmp/claude-plan-${REVIEW_ID}.md` with the revised version.
2. **Briefly summarize** what you changed for the user:

```
### Revisions (Round N)
- [What was changed and why, one bullet per Codex issue addressed]
```

3. Inform the user what's happening: "Sending revised plan back to Codex for re-review..."

### Step 6: Re-submit to Codex (Rounds 2-5)

Resume the existing Codex session so it has full context of the prior review:

```bash
codex exec resume ${CODEX_SESSION_ID} \
  "I've revised the plan based on your feedback. The updated plan is in /tmp/claude-plan-${REVIEW_ID}.md.

Here's what I changed:
[List the specific changes made]

Please re-review. If the plan is now solid and ready to implement, end with: VERDICT: APPROVED
If more changes are needed, end with: VERDICT: REVISE" 2>&1 | tail -80
```

**Note:** `codex exec resume` does NOT support `-o` flag. Capture output from stdout instead (pipe through `tail` to skip startup lines). Read the Codex response directly from the command output.

Then go back to **Step 4** (Read Review & Check Verdict).

**Important:** If `resume ${CODEX_SESSION_ID}` fails (e.g., session expired), fall back to a fresh `codex exec` call with context about the prior rounds included in the prompt.

### Step 7: Present Final Result

Once approved (or max rounds reached):

```
## Codex Review — Final (model: gpt-5.3-codex)

**Status:** ✅ Approved after N round(s)

[Final Codex feedback / approval message]

---
**The plan has been reviewed and approved by Codex. Ready for your approval to implement.**
```

If max rounds were reached without approval:

```
## Codex Review — Final (model: gpt-5.3-codex)

**Status:** ⚠️ Max rounds (5) reached — not fully approved

**Remaining concerns:**
[List unresolved issues from last review]

---
**Codex still has concerns. Review the remaining items and decide whether to proceed or continue refining.**
```

### Step 8: Cleanup

Remove the session-scoped temporary files:
```bash
rm -f /tmp/claude-plan-${REVIEW_ID}.md /tmp/codex-review-${REVIEW_ID}.md
```

## Loop Summary

```
Round 1: Claude sends plan → Codex reviews → REVISE?
Round 2: Claude revises → Codex re-reviews (resume session) → REVISE?
Round 3: Claude revises → Codex re-reviews (resume session) → APPROVED ✅
```

Max 5 rounds. Each round preserves Codex's conversation context via session resume.

## Rules

- Claude **actively revises the plan** based on Codex feedback between rounds — this is NOT just passing messages, Claude should make real improvements
- Default model is `gpt-5.3-codex`. Accept model override from the user's arguments (e.g., `/codex-review o4-mini`)
- Always use read-only sandbox mode — Codex should never write files
- Max 5 review rounds to prevent infinite loops
- Show the user each round's feedback and revisions so they can follow along
- If Codex CLI is not installed or fails, inform the user and suggest `npm install -g @openai/codex`
- If a revision contradicts the user's explicit requirements, skip that revision and note it for the user